
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119541534-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119541534-1');
</script>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Patterson Consulting: Blog - Real-Time Sensor Data Processing with the Kafka Streaming API</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="blog page for Patterson Consulting" />
	<meta name="keywords" content="blog, patterson consulting, deep learning, machine learning, apache hadoop, apache spark, etl, consulting" />
	<meta name="author" content="Patterson Consulting" />

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<!-- <link rel="shortcut icon" href="favicon.ico"> -->

	<link rel="stylesheet" href="../css/animate.css">
	<link rel="stylesheet" href="../css/bootstrap.css">
	<link rel="stylesheet" href="../css/icomoon.css">

	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">

	<link rel="stylesheet" href="../css/style.css">

	<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
	<style>
		a {
			color: #FF0000;
			text-decoration: underline;
		}

	</style>

	<script src="../js/modernizr-2.6.2.min.js"></script>
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body class="boxed">
	<!-- Loader -->
	<div class="fh5co-loader"></div>

	<div id="wrap">

	<div id="fh5co-page">
		<header id="fh5co-header" role="banner">
			<div class="container">
				<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle dark"><i></i></a>
				<div id="fh5co-logo"><a href="index.html"><img src="../images/website_header_top_march2018_v0.png" ></a></div>
				<nav id="fh5co-main-nav" role="navigation">
					<ul>
						<li><a href="../about.html">About</a></li>
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Services</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../big_data_apps.html">Hadoop Applications</a></li>
										<li><a href="../vision_apps.html">Computer Vision Applications</a></li>
										<li><a href="../sensor_apps.html">Sensor Applications</a></li>
										<li><a href="../exec_strategy.html">Executive Strategy</a></li>

									</ul>
								</div>
							</div>
						</li>
						<!--
						<li><a href="portfolio.html">Portfolio</a></li>
-->
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Technologies</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../deep_learning.html">Deep Learning</a></li>
										<li><a href="../hadoop.html">Apache Hadoop</a></li>
									</ul>
								</div>
							</div>
						</li>

						<li class="cta"><a href="../contact.html">Contact</a></li>
					</ul>
				</nav>
			</div>
		</header>
		<!-- Header -->

		<div id="fh5co-intro" class="fh5co-section">
			<div class="container">


				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h1>Real-Time Sensor Data Processing with the Kafka Streaming API</h1>
						<p>Author: <a href="https://twitter.com/Austin_P_Harris">Austin Harris</a></p>
						<p>Date: July 26th, 2018</p>

						<p>
							<i>In this blog article we aim to introduce you to the Kafka Streams API by creating our own stream processing application.</i>
						</p>
							<p>With today's rapid adoption of sensors and IOT devices the demand for real-time processing applications has exploded. Large deployments of connected devices require specialized infrastructure in order to ingest large volumes of high frequency data. </p> <p> In this article, we will use Kafka to set up a data pipeline in order to ingest streaming data from multiple simulated sensors, and create an application using the Kafka Streams API to process the data in real-time. This articles primary objective is to introduce Kafka Streams API. Therefore, we assume you have a Kafka broker up and running. If you are new to Kafka, I reccommend starting <a href="https://docs.confluent.io/current/platform.html">here</a>.
							</p>
						</p>
					</div>
				</div>




				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">

							<!-- <script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_tf_object_detection/blob/master/src/main/java/com/pattersonconsultingtn/kafka/examples/tf_object_detection/vision/TFVision_ObjectDetection.java?slice=95:140"></script> -->

						<h2>What is Stream Processing?</h2>

						<p>Before we can define stream processing, we need to make sure we know what a stream is. While many explanations exist, we will be using the one commonly agreed upon:

							<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
							  <p style="font-size: 14px;">
							  <i>"First and foremost, a data stream is an abstraction representing an
										unbounded dataset. Unbounded means infinite and ever growing. The dataset is
										unbounded because over time, new records keep arriving. This definition is used by
										Google, Amazon, and pretty much everyone else."</i></p>
							  <p><a href="http://shop.oreilly.com/product/0636920044123.do">Narkhede, Shapira, and Palino, "Kafka: The Definitive Guide"</a></p>
							</blockquote>

						<p>Stream processing is the processing of data continuously, a single record at a time. Understanding events in real-time can greatly benefit business decision making skills.<p>

						<p>
							A few examples of streams would include:
							<ol>
								<li>network events</li>
								<li>credit-card transactions</li>
								<li>events from sensors</li>
							</ol>
						</p>








						<!-- <p>
						We can see an example of an avro schem below:

							<script src="http://gist-it.appspot.com/https://github.com/confluentinc/kafka-streams-examples/blob/4.1.1-post/src/main/resources/avro/io/confluent/examples/streams/pageview.avsc"></script>

						</p> -->

						<!-- <p>
							We can see from the Avro example above that there are four fields (outside of the fields themselves in the "fields" entry):

							<ol>
								<li>type - Identifies the JSON field type. For Avro schemas, this must always be 'record' when it is specified at the schema's top level</li>
								<li>namespace - effectively a URI that uniquely identifies this schema for your organization</li>
								<li>name - combines with namespace (making it fully-qualified) to uniquely identify in the global pool of Avro schemas</li>
								<li>fields - actual schema definition, defines what fields are in the value payload of the message and their corresponding data types</li>

							</ol>



						Note that schema field names must begin with [A-Za-z_], and subsequently contain only [A-Za-z0-9_].

						</p> -->
					</div>
				</div>
				<!-- end of section -->

				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Kafka Streams</h2>
						<p> Kafka Streams is a client library for stream processing applications that makes writing scalable, fault-tolerant streaming applications easy. It requires no other dependencies other than kafka itself. A streams application can be package, and deployed like any other Java application. A kafka streams application processes data in the cluster as the input topic and then outputs its results back into the cluster as a new topic.</P>
						<p>For now, we will use the high-level Streams DSL (Domain Specific Language) to create our application. However, Kafka does provide a Low-Level Processor API if you need a bit more control. If you would like to learn more about the APIs themselves <a href="https://www.confluent.io">Confluent</a> provides some great <a href="https://docs.confluent.io/3.0.0/streams/developer-guide.html">examples</a>. We will use windowed aggregation in order to show how easy it is to compute a moving average in the example below.</p>

						<h3>Example</h3>

						<p> All code in this article can be found at: </p>
						<p>In order to introduce the Kafka Streams API, we will implememnt an example Kafka streams application that will calculate the moving average of a given topic. Each topic will consist of a single stream of data from a sensor. This example is broken down into three main components:
							<ul>
								<li>Sensor Stream Producer</li>
								<li>Sensor Stream Processor</li>
								<li>Sensor Stream Aggregator</li>
							</ul>
						</p>

						<p>The Stream Producers will publish readings to sensor topics. The stream processor will read from a single topic, and calculate the moving average for the given topic. In order to do so, we will implement our own aggregator that will be used to keep track of the number of readings, and the sum in order to compute the average.</p>

						<h3>Stream Producer</h3>

						<p>Since we do not have access to sensors, we will simulate our own. We will use a subset of data from UCI’s “Gas sensors for home activity monitoring”. You can download the dataset here. </p>
						<p>Data from each sensor will be published via the producer to the sensors topic. The topic is determined by the sensors type. Each sensor will have its own topic. The date, time, sensor type and reading make up a single messages. Avro is used for serialization and deserialization in this example, if you are new to Avro I would suggest checking out this <a href="http://www.pattersonconsultingtn.com/blog/avro_101_w_kafka.html">article</a>. The schema is shown below: </p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/PollutionDataTracker/PollutionSensorProducer.java?slice=70:79"></script>

						<p>Next, configure your Producer properties:</p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/PollutionDataTracker/PollutionSensorProducer.java?slice=50:57"></script>

						<p>Lastly, for each record in the CSV, publish a message for each sensor consisting of the sensor reading, type, date and time. This is shown in the code below: </p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/PollutionDataTracker/PollutionSensorProducer.java?slice=94:111"></script>

						<h3>Sensor Aggregator</h3>

						<p> The sensor aggregator is responsible for defining how the data should be aggregated. The aggregator consists of two methods: <code>add()</code> and <code>calculateAverage()</code>. The average is calculated for the duration of pre-defined windows of time. For a given time window, an aggregator is initialized and the readings that are recieved are passed via the <code>add()</code> method. The <code>calculateAverage()</code> method is called when the time window has completed. </p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/AirQualityProcessor/SensorAggregator.java?slice=5:32"></script>

						<h3>Sensor Processor</h3>

						<p>The SensorStreamProcessor uses the Kafka Streams API windowed aggregation to aggregate sensor data into a new stream. First, a StreamBuilder is initialized and a KStream is created using <code>builder.stream(“topicName”)</code>. The “topicName” can be any of the sensor types at the top of the CSV file. Here we build a stream from the "PT08S1" sensor:</p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/AirQualityProcessor/SensorStreamProcessor.java?slice=56:59"></script>

						<p>Now that we have the stream we use the aggregate() method along with our sensor aggregator, then we specify the window size, and then calculate the average for each window.</p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/AirQualityProcessor/SensorStreamProcessor.java?slice=63:71"></script>

						<p>If you would like to view the new stream, you can use the code below to print off each message in your aggregated stream:</p>

						<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_streaming_sensor_analysis/tree/master/src/main/java/com/pattersonconsultingtn/kafka/examples/AirQualityProcessor/SensorStreamProcessor.java?slice=71:78"></script>

					<h2>Running The Example</h2>
					<p>To run this example, ZooKeeper needs to be up and running. Run: </p>
					<p>
					<code>./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties</code>
				</p>
					<p>Next, start the Kafka Broker: </p>
					<p>
						<code> ./bin/kafka-server-start ./etc/kafka/server.properties</code>
					</p>

					<p>Once these are up and running, you will need to create your topic. Run the command below to create the topic for a single sensor. While our producer does create a stream for each sensor, the stream processor application requires only one sensor. We will use the "PT08S1" sensor. Run: </p>
					<p><code> ./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 \
 --partitions 1 --topic PT08S1</code></p>

 					<p>Now run our stream processor:</p>
					<p><code>mvn exec:java -Dexec.mainClass="com.pattersonconsultingtn.kafka.examples.AirQualityProcessor.SensorStreamProcessor"</code></p>
					<p>Lastly, start our stream producer:</p>
					<p><code>mvn exec:java -Dexec.mainClass="com.pattersonconsultingtn.kafka.examples.PollutionDataTracker.PollutionSensorProducer"</code></p>



						<!-- <p>
							We need the <a href="https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java">serializer</a> used to produce messages to the Kafka cluster to match the <a href="https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroDeserializer.java">deserializer</a> that will be used when consuming messages (otherwise bad things happen and <a href="https://github.com/confluentinc/schema-registry/blob/master/core/src/main/java/io/confluent/kafka/schemaregistry/storage/exceptions/SerializationException.java">exceptions</a> are thrown).

							The behind-the-scenes details of storing the schema in the registry and pulling it up when required is performed by the serializers and deserializers in Kafka (which is pretty handy). Having to manually keep track of these when dealing with lots of concurrent streams on a well-worked Kafka cluster quickly becomes a challenge. Being able to leverage Avro in conjunction with the Confluent Schema Registry solves a lot of issues easily that most enterprises don't take a lot of time to think about (side note: data platforms in the enterprise commonly overlook many of the small issues like these which can end up taking most of the engineering time, ironically enough).

						</p> -->

			</div>
		</div>





	</div>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-chevron-down"></i></a>
	</div>

	<script src="../js/jquery.min.js"></script>
	<script src="../js/jquery.easing.1.3.js"></script>
	<script src="../js/bootstrap.min.js"></script>
	<script src="../js/owl.carousel.min.js"></script>
	<script src="../js/main.js"></script>

	</body>
</html>
