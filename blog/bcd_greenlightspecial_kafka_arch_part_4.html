
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119541534-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119541534-1');
</script>
		
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Patterson Consulting: Blog - Real-Time Analysis of Computer Vision Objects with TensorFlow and Apache Kafka</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="blog page for Patterson Consulting" />
	<meta name="keywords" content="blog, patterson consulting, deep learning, machine learning, apache hadoop, apache spark, etl, consulting" />
	<meta name="author" content="Patterson Consulting" />

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<!-- <link rel="shortcut icon" href="favicon.ico"> -->
	
	<link rel="stylesheet" href="../css/animate.css">
	<link rel="stylesheet" href="../css/bootstrap.css">
	<link rel="stylesheet" href="../css/icomoon.css">

	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">

	<link rel="stylesheet" href="../css/style.css">

	<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
	<style>
		a { 
			color: #FF0000; 
			text-decoration: underline;
		}

	</style>

	<script src="../js/modernizr-2.6.2.min.js"></script>
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body class="boxed">
	<!-- Loader -->
	<div class="fh5co-loader"></div>

	<div id="wrap">

	<div id="fh5co-page">
		<header id="fh5co-header" role="banner">
			<div class="container">
				<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle dark"><i></i></a>
				<div id="fh5co-logo"><a href="index.html"><img src="../images/website_header_top_march2018_v0.png" ></a></div>
				<nav id="fh5co-main-nav" role="navigation">
					<ul>
						<li><a href="../about.html">About</a></li>
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Services</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../offerings/data_science_offerings.html">Data Science Offerings</a></li>
										<li><a href="../big_data_apps.html">Hadoop Applications</a></li>
										<li><a href="../vision_apps.html">Computer Vision Applications</a></li>
										<li><a href="../sensor_apps.html">Sensor Applications</a></li>
										<li><a href="../exec_strategy.html">Executive Strategy</a></li>

									</ul>
								</div>
							</div>
						</li>
						<!--
						<li><a href="portfolio.html">Portfolio</a></li>
-->
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Technologies</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../deep_learning.html">Deep Learning</a></li>
										<li><a href="../hadoop.html">Apache Hadoop</a></li>										
									</ul>
								</div>
							</div>
						</li>
						
						<li><a href="../blog/blog_index.html">Blog</a></li>
					
						<li class="cta"><a href="../contact.html">Contact</a></li>
					</ul>
				</nav>
			</div>
		</header>
		<!-- Header -->
		
		<div id="fh5co-intro" class="fh5co-section">
			<div class="container">


				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h1>Building the Next-Gen Retail Experience with Apache Kafka and Computer Vision (Part 3)</h1>
						<p>Author: <a href="http://www.twitter.com/jpatanooga">Josh Patterson</a></p>
						<p>Date: July 16th, 2018</p>
						
						<p>
							<i>In this blog article we take on the perspective of the enterprise development team of a fictional Fortune 500 Retailer "Big Cloud Dealz" looking to integrate emerging technology to re-invent the in-store customer experience. We wanted to tell the story of how the <a href="../offerings/data_science_offerings.html">Patteson Consulting team would help</a> a fictional enterprise navigate the evolution towards digital transformation.</i>

						</p>


					</div>
				</div>
				<!-- end of section -->


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Ingesting the Inventory MySQL Table with Kafka Connect</h2>

						<p>
							<i>This is part 4 in a 5-part blog series on building the "Green Light Special" system for fictional retailer, Big Cloud Dealz.</i>

						</p>

						<p>

							In part 3 of our series, we had just aggregated all of the incoming object detections into a new topic <code>aggregate_cart_objects</code>.

							We need to join this aggregated information with the current inventory to know which items are paired to upsell by the business analysis group (e.g., "business rules"). The inventory information needs to be joined with the new object detections as they come into the Kafka cluster, so latency is important as well.

						</p>

						<div style="float: right; margin: 12px; border: 0px solid #999999;">

							<img src="./images/bcd_arch_part_4.png" style="width: 665px; height: 396px;" />

						</div>

						<p>
							Big Cloud Dealz's inventory table is located in a relational database which is a typical home for this type of inforamtion in most Fortune 500 companies. They are using specifically the MySQL database to house their inventory information. Given that we don't want to constantly perform RDBM queries to rebuild the joined data as new information comes in (this would create <a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/">excessive table lookups, causing scalability issues</a>), we need to periodically cache the latest inventory table in Kafka as a topic. We'll do this with the <a href="https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/">kafka Connect tool</a> which is included in Confluent's Kafka platform. We'll also again leverage the schema registry to standardize our data with the <A href="http://www.pattersonconsultingtn.com/blog/avro_101_w_kafka.html">Avro</a> message format, as this proves advantageous for managing schemas over time:


							
						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"Although the Schema Registry is not a required service for Kafka Connect, it enables you to easily use Avro as the common data format for all connectors. This keeps the need to write custom code at a minimum and standardizes your data in a flexible format. Additionally, you get the benefits of enforced compatibility rules for schema evolution."</i></p>
						  <p><a href="https://docs.confluent.io/current/connect/userguide.html">Kafka Connect User Guide</a> from Confluent Documentation</p>
						</blockquote>

						With context now set, let's dig into how we will use Kafka Connect to cache the MySQL table in Kafka as a topic.




						</p>


						<p>
							<h3>Preparing the MySQL Inventory Table</h3>

							For the purposes of this example we'll assume you either have MySQL working locally or can <a href="https://www.mysql.com/">install it</a> on your own.

							Once you've logged into MySQL, use the following script to build the inventory table and populate it with inventory data that we'll use later on in this blog series.
						</p>
						<p>


			<pre><span style="font-weight: 400; font-size: 12px;">mysql> CREATE DATABASE big_cloud_dealz;
mysql> USE big_cloud_dealz;

mysql> CREATE TABLE inventory (
 id serial NOT NULL PRIMARY KEY,
 name varchar(100),
 upsell_item varchar(200),
 count INT,
 modified timestamp default CURRENT_TIMESTAMP NOT NULL,
 INDEX `modified_index` (`modified`)
 );

mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('cup', 'plate', 100);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('bowl', 'cup', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('fork', 'spoon', 200);

mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('spoon', 'fork', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('sportsball', 'soccer goal', 2);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('tennis racket', 'tennis ball', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('frisbees', 'frisbee goal', 100);

</span></pre>							

						</p>
						<p>

							The above commands will do 4 things:
							<ol>
								<li>Creates a database in MySQL called <code>big_cloud_dealz</code></li>
								<li>Switches the current database to <code>big_cloud_dealz</code> in the MySQL command line tool.</li>
								<li>Creates a table called <code>inventory</code> in the <code>big_cloud_dealz</code> database.</li>
								<li>Inserts 7 records in the <code>inventory</code> table that we'll use in this example.</li>

							</ol>

							Now let's move on to configuring Kafka Connect so we can ingest the <code>inventory</code> table into a topic in the Kafka cluster.

						</p>
						<p>
							<h3>Configuring Kafka Connect</h3>

							Integrating data systems is an age old problem in enterprise architecture. Writing custom code to connect a new system to Kafka begins to create a high development and maintenance cost for teams, so we want standardized ways to integrate systems. <a href="https://docs.confluent.io/current/connect/index.html">Kafka Connect</a> let's us standardize how we move data into and out of Kafka and should be our first choice when connecting to a new system. 

						</p>
						<p>
							Kafka Connect helps create reliable, high-performance ETL pipelines and we'll focus on ingesting the <code>inventory</code> table using it.
							In this situation we need to ingest our <code>inventory</code> table into a topic in the Kafka cluster. The Kafka Connect system will use a pre-defined connector to communicate with MySQL and ingest an Avro message for every record in the table.

						</p>
						<p>


							Given that our system is based on the Confluent platform for Kafka, we already have Kafka Connect setup. Before we startup Kafka Connect we need to configure it to know where our database is located, what information to ingest, and how to connect to it. We can see the conf file for the Kafka Connect system below.
						</p>
						<p>


			<pre><span style="font-weight: 400; font-size: 12px;"># Bootstrap Kafka servers. If multiple servers are specified, they should be comma-separated.
bootstrap.servers=localhost:9092

# The converters specify the format of data in Kafka and how to translate it into Connect data.
# Every Connect user will need to configure these based on the format they want their data in
# when loaded from or stored into Kafka
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://localhost:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081

# The internal converter used for offsets and config data is configurable and must be specified,
# but most users will always want to use the built-in default. Offset and config data is never
# visible outside of Connect in this format.
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# Local storage file for offset data
offset.storage.file.filename=/tmp/connect.offsets

plugin.path=share/java,/Users/josh/Documents/workspace/PattersonConsulting/confluent/mysql-connector-java-8.0.12</span></pre>
						</p>
						<p>

							This configuration file tells Connect where the boostrap server for the Kafka cluster lives, to use Avro for the messages, and where our connector plugin jars live. Let's now move on and take a look at how we configure the MySQL connector as a message source.



						</p>
						<p>
							<h3>Configuring the MySQL Source Connector</h3>

							We'll use the stock JDBC connector that ships with the Confluent platform. The stock JDBC connector allows us to connect to any relational database that supports JDBC as described below:
							
						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"The JDBC connector allows you to import data from any relational database with a JDBC driver (such as MySQL, Oracle, or SQL Server) into Kafka. By using JDBC, this connector can support a wide variety of databases without requiring custom code for each one."</i></p>
						  <p>Confluent Blog Post: <a href="https://www.confluent.io/blog/how-to-build-a-scalable-etl-pipeline-with-kafka-connect/">How to Build a Scalable ETL Pipeline with Kafka Connect</a></p>
						</blockquote>

						The most complicated parst of configuring the connector to talk to MySQL is getting the connection string right. We have a few snafus getting it correctly connected (looking at you, <code>useJDBCCompliantTimezoneShift=true</code>). We share our Kafka Connect MySQL connector conf file below:
						</p>
						<p>

			<pre><span style="font-weight: 400; font-size: 12px;">name=test-mysql-jdbc
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
connection.url=jdbc:mysql://localhost:3306/big_cloud_dealz?user=root&password=1234&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC
mode=timestamp+incrementing
incrementing.column.name=id
timestamp.column.name=modified
topic.prefix=mysql_tables_jdbc_
table.whitelist=inventory</span></pre>
							
						</p>
						<p>
							This connector is setup for standalone (non-distributed) mode, and you'll likely spend the most time making sure your <Code>connection.url</code> jdbc connection string is correct. Once we have this conf file ready, we can move on to using the 2 configuration files to crank up Kafka Connect and get the <code>inventory</code> table loaded as a topic.



						</p>
						<p>
							<h3>Running Kafka Connect to Export the Inventory Table From MySQL</h3>


							To run Kafka Connect, we need the following components running:

							<ol>
								<li>Zookeeper</li>
								<li>Kafka Broker</li>
								<li>Schema Registry</li>
								<li>MySQL Server</li>

							</ol>

							With these daemons running, we can then start Kafka Connect to extract the <code>inventory</code> table from the MySQL database into the Kafka topic <Code>mysql_tables_jdbc_inventory</code> with the command:

						</p>
						<p>

			<pre><span style="font-weight: 400; font-size: 12px;">$ bin/connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties ...]</span></pre>	
						</p>					
						<p>


			<pre><span style="font-weight: 400; font-size: 12px;">$ connect-standalone /mnt/etc/connect-avro-standalone.properties \
    /mnt/etc/mysql.properties /mnt/etc/hdfs.properties &amp;</span></pre>	

						</p>

						<p>
							This command will output logs to the terminal similar to what we see below:

						</p>
						<p>

<consoleoutput>[2018-08-03 10:25:52,822] INFO WorkerSourceTask{id=test-mysql-jdbc-0} Finished commitOffsets successfully in 10 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:427)
[2018-08-03 10:26:52,826] INFO WorkerSourceTask{id=test-mysql-jdbc-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:328)
[2018-08-03 10:26:52,827] INFO WorkerSourceTask{id=test-mysql-jdbc-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:345)
[2018-08-03 10:27:52,826] INFO WorkerSourceTask{id=test-mysql-jdbc-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:328)
[2018-08-03 10:27:52,826] INFO WorkerSourceTask{id=test-mysql-jdbc-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:345)
[2018-08-03 10:28:52,826] INFO WorkerSourceTask{id=test-mysql-jdbc-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:328)
[2018-08-03 10:28:52,827] INFO WorkerSourceTask{id=test-mysql-jdbc-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:345)
</consoleoutput>
						</p>

						<p>

							If we launch another terminal window we can query our inventory data in the <code>mysql_tables_jdbc_inventory</code> topic with the following command:


						</p>
						<p>
			<pre><span style="font-weight: 400; font-size: 12px;">$ ./bin/kafka-avro-console-consumer --zookeeper localhost:2181 --topic mysql_tables_jdbc_inventory --from-beginning</span></pre>
						</p>
						<p>
							The output should look like we see in below:


						</p>

						<p>

							
<!--
<pre><span style="font-weight: 400; font-size: 12px;">
-->
<consoleoutput>
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
{"id":1,"name":{"string":"cup"},"upsell_item":{"string":"plate"},"count":{"int":100},"modified":1533291848000}
{"id":2,"name":{"string":"bowl"},"upsell_item":{"string":"cup"},"count":{"int":10},"modified":1533291848000}
{"id":3,"name":{"string":"fork"},"upsell_item":{"string":"spoon"},"count":{"int":200},"modified":1533291848000}
{"id":4,"name":{"string":"spoon"},"upsell_item":{"string":"fork"},"count":{"int":10},"modified":1533291848000}
{"id":5,"name":{"string":"sportsball"},"upsell_item":{"string":"soccer goal"},"count":{"int":2},"modified":1533291848000}
{"id":6,"name":{"string":"tennis racket"},"upsell_item":{"string":"tennis ball"},"count":{"int":10},"modified":1533291848000}
{"id":7,"name":{"string":"frisbees"},"upsell_item":{"string":"frisbee goal"},"count":{"int":100},"modified":1533291848000}
</consoleoutput>
<!--
</span></pre>							
-->
						</p>

						<p>
							<h3>Summary</h3>
							At this point we have our system detecting objects, sending them to a Kafka topic, and then aggregating them into a topic. We also have our inventory table from MySQL being ingested into its own Kafka topic. In our final post in this series, the Big Cloud Dealz team will join the aggregated cart items with the inventory table in real-time to create the "Green Light Special" application. Hopefully, Big Cloud Ron will approve.



						</p>
						
						


					</div>
				</div>
				<!-- end of section -->





				<div style="border-top: 1px #999999 solid;"> </div>				
				<br/>
				<br/>
				<br/>

				[ start 5th blog post ]		


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Joining the Aggregated Objects with the Inventory Table with the Streaming API</h2>

						<p>
							In the last article we setup Kafka Connect to import the <code>inventory</code> table from a MySQL database and store it as a topic in the Kafka cluster.

							In this article we'll put the final peice in place and the Big Cloud Dealz team will join the aggregated cart items with the inventory table in real-time to create the "Green Light Special" application.
						</p>
						<p>
													
							<img src="./images/bcd_arch_part_5.png" style="width: 918px; height: 400px;" />
						</p>
						<p>
							In the diagram above we can see:

							<ol>
								<li>The shopping cart sending detected objects as message via the producer API to the Kafka cluster topic <code>shopping_cart_objects</code></li>
								<li>A new aggregate real-time view of the detected objects across all carts being generated with the streaming API and written to the topic <code>aggregate_cart_obects</code></li>
								<li>Kafka Connect separate from the Kafka cluster of brokers connecting to the MySQL database via a JDBC connector and ingesting the <code>inventory</code> table into the <code>inventory</code> topic in the Kafka cluster.</li>

							</ol>

							We can also see the join we're going to build between the <code>aggregate_cart_objects</code> topic and the <code>inventory</code> topic in the diagram above to produce the <code>top_cart_upsells</code> topic. This join is the focus of part 5 of this blog series, and also a narrative seen commonly in the land of real-time applications as expressed in the excerpt below:


						</p>



						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"I’ll argue that the fundamental problem of an asynchronous application is combining tables that represent the current state of the world with streams of events about what is happening right now."</i></p>
						  <p>Jay Kreps' Blog Article: <a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/">"Introducing Kafka Streams: Stream Processing Made Simple"</a></p>
						</blockquote>

						<p>
							With that, let's dig into building this streaming DSL join by reviewing the schemas for the topics that we want to join.

						</p>

						<p>
							<h3>Topic Schemas to Join</h3>

							As outlined in the diagram above, we want to join the <code>aggregate_cart_objects</code> topic and the <code>inventory</code> topic to produce the final <code>top_cart_upsells</code> topic.

							To do this let's review each topic schema in preparation of writing the join code. Let's take a look at the Avro schema generated by Kafka Connect that we setup previously in the last blog post. To do that we'll use the <a href="https://docs.confluent.io/current/schema-registry/docs/using.html">schema registry web API</a> with the command line <kbd>curl</kbd> command to get a listing of all of the schemas in the registry:

						</p>

						<p>

							<pre><span style="font-weight: 400; font-size: 12px;">$ curl -X GET http://localhost:8081/subjects</span></pre>

						</p>
						<p>
							This command will give us console output similar to what we see below:

						</p>


						<p>

<consoleoutput>["mysql_tables_jdbc_inventory-value"]</consoleoutput>

						</p>

						<p>
							As we can see the topic name includes the <code>-value</code> suffix on the end of the topic prefix <Code>topic.prefix=mysql_tables_jdbc_</code> specified in the configuration file. We do not see an entry for the topic key in the schema registry because table data exported from a database with the JDBC connector and Kafka Connect <a href="https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-3/">will have a null key by default</a>.
						</p>
						<p>
							<h4>Topic mysql_tables_jdbc_inventory Message Schema</h4>

							To view the <code>mysql_tables_jdbc_inventory-value</code> schema stored by Kafka Connect in the schema registry we'll use the web API again and the <kbd>curl</kbd> command to call the schema registry REST API:

						</p>

						

						<p>

							<pre><span style="font-weight: 400; font-size: 12px;">$ curl -X GET http://localhost:8081/subjects/mysql_tables_jdbc_inventory-value/versions/1</span></pre>

						</p>

						<p>

							The console output will look a bit jumbled together, so let's take a look at the schema JSON text pretty formatted:

						</p>
						<p>

<pre><code>{
  "schema": 
  "{
	  \"type\":\"record\",
	  \"name\":\"inventory\",
	  \"fields\":
	  [
	    {\"name\":\"id\",\"type\":\"long\"},
	    {\"name\":\"name\",\"type\":[\"null\",\"string\"],\"default\":null},
	    {\"name\":\"upsell_item\",\"type\":[\"null\",\"string\"],\"default\":null},
	    {\"name\":\"count\",\"type\":[\"null\",\"int\"],\"default\":null},
	    {\"name\":\"modified\",\"type\":
	    	{
	    		\"type\":\"long\",
	    		\"connect.version\":1,
	    		\"connect.name\":\"org.apache.kafka.connect.data.Timestamp\",
	    		\"logicalType\":\"timestamp-millis\"
	    	}
	    }
	  ],
	  \"connect.name\":\"inventory\"
	}"
}
</code></pre>


						</p>		
						<p>
							
							The Kafka Connect extracted mysql <code>inventory</code> table column names from the schema above are shown below for clarity:

							<table class="table" style="border: 2px solid #999999;">
								<tr>
									<th>Column Name</th>
									<th>Type</th>
									<th>Description</th>

								</tr>

								<tr>
									<td>id</td>
									<td><code>Long</code></td>
									<td><i>Record ID</i></td>
								</tr>

								<tr>
									<td>name</td>
									<td><code>String</code></td>
									<td><i>Name of the item in inventory</i></td>
								</tr>

								<tr>
									<td>upsell_item</td>
									<td><code>String</code></td>
									<td><i>Name of the item that we want to upsell paired with this item</i></td>
								</tr>

								<tr>
									<td>count</td>
									<td><code>Int</code></td>
									<td><i>Current count of this item in inventory</i></td>
								</tr>

								<tr>
									<td>modified</td>
									<td><code>Long</code></td>
									<td><i>Last modified timestamp</i></td>
								</tr>


							</table>
						</p>
						<p>
							While the table above describes the message schema of the topic, we'll point out that our default key for the topic is <code>null</code> as we previously mentioned. This will come into play in a moment as we design our streaming join.

						</p>
						<p>

							<h4>Topic aggregate_cart_objects Message Schema</h4>

							Let's now remember the <code>aggregate_cart_objects</code> topic we created previously in this blog series. It has a single field in the <b>message schema</b> as shown in the table below:
						</p>
						<p>

							<table class="table" style="border: 2px solid #999999;">
								<tr>
									<th>Column Name</th>
									<th>Type</th>
									<th>Description</th>

								</tr>
								<!--
								<tr>
									<td>class_name</td>
									<td><code>String</code></td>
									<td><i>Name of the detected object, will match up to a name in the inventory</i></td>
								</tr>
							-->

								<tr>
									<td>count</td>
									<td><code>Long</code></td>
									<td><i>Number of times this item was detected across all baskets</i></td>
								</tr>


							</table>							

<!--
							This avro schema was generated automatically when we created the <code>aggregate_cart_objects</code> topic and then wrote to it from our streaming code in part 3 of this blog series. 

							In the code snippet below we can see how in the Kafka configuration portion of the application code we set <Code>DEFAULT_KEY_SERDE_CLASS_CONFIG</code> to <code>String</code> and <code>DEFAULT_VALUE_SERDE_CLASS_CONFIG</code> to <code>GenericAvroSerde</code>. 
-->
							</p>
							<p>

							</p>
							The topic is keyed on the <code>String</code> name of the object class and the topic does not show up in the schema registry. How did this play out differently?

							<p>
			

If we check the schema registry, we do not see an entry for this topic. This is because when we first wrote to the topic in the streaming application <code>StreamingDetectedObjectCounts</code> we manually specified the Serdes for both the key (<code>Serdes.String()</code>) and the value (<code>Serdes.Long()</code>) of the topic with the code:
						</p>

<pre><code>detectedObjectCountsStream.to( aggregateDestTopicName, Produced.with(Serdes.String(), Serdes.Long()));
</code></pre>
<!--
<pre><code>props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, GenericAvroSerde.class);
</code></pre>
-->

						<p>
							This means we keyed the topic on the class name of the object, and the message schema is simply the <code>Long</code> count of the objects from the aggregate streaming code.

						</p>			

						<p>
							<h3>Output Topic Schema</h3>


						

							Logically we want to join on <code>mysql_tables_jdbc_inventory.name</code> and <code>aggregate_cart_objects.class_name</code> in our code to produce the <code>top_cart_upsells</code> topic. The information we want to send to our sales associates on the Big Cloud Dealz floor with the <code>top_cart_upsells</code> topic logically looks like:
						</p>

							<table class="table" style="border: 2px solid #999999;">
								<tr>
									<th>Column Name</th>
									<th>Type</th>
									<th>Description</th>

								</tr>
								<tr>
									<td>item_name</td>
									<td><code>String</code></td>
									<td><i>the name of the item in the shopper's cart</i></td>
								</tr>

								<tr>
									<td>upsell_item_name</td>
									<td><code>String</code></td>
									<td><i>the name of the item that the business analysts paired with this item for upsell</i></td>
								</tr>

								<tr>
									<td>item_cart_count</td>
									<td><code>Long</code></td>
									<td><i>the count of this item across all carts on the floor</i></td>
								</tr>


							</table>

						<p>

							Now that we have an output schema in mind, let's design a join with the Kafka Streaming DSL to produce these messages with this schema and write them back to the <code>top_cart_upsells</code> topic.


						</p>							


					</div>
				</div>
				<!-- end of section -->				


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Designing a Streaming Join with the Kafka Streaming API</h2>

						<p>

							Our streaming application to finish the "Green Light Special" application will involve a real-time streaming join between the following topics:

							<ol>
								<li>The aggregated detected objects from the cart in the topic <code>aggregate_cart_objects</code>.</li>
								<li>The inventory we want to join against is represented (from the previous post) in the <code>mysql_tables_jdbc_inventory</code> topic.</li>

							</ol>



							

							The stream of aggregated cart item updates (<code>aggregate_cart_objects</code>) are interpreted as a <b>changelog</b> stream, where each new detected object message represents an update (i.e. any previous data records having the same record key will be replaced by the latest update in the aggregate topic; <a href="https://www.confluent.io/blog/building-real-time-streaming-etl-pipeline-20-minutes/">Confluent blog article</a> for more context). So we'll represent the <code>aggregate_cart_objects</code> topic as a <code>KTable</code> in the Kafka Streams DSL.


						</p>

						<p>
							If we we're doing the join with the raw incoming object detections as opposed to the aggregated results, we'd use a <code>KStream</code> as each record would represent a self-contained datum (e.g., similar to a "click stream from a website"). We can summarize this delineation as described in the quote from the Confluent blog below:

						</p>

						<p>


						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"In Kafka Streams, a record stream is represented via the so-called <code>KStream</code> interface and a changelog stream via the <code>KTable</code> interface."</i></p>
						  <p>Confluent Blog Article: <a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/">"Distributed, Real-time Joins and Aggregations on User Activity Events using Kafka Streams"</a></p>
						</blockquote>




						</p>
						<p>
							The <code>mysql_tables_jdbc_inventory</code> topic is a stream of updates from MySQL so we'll represent it as a <code>KStream</code>.

						</p>

						<p>

							Now that we've decided how we'll represent the inputs to our streaming join, we need to decide on:

							<ol>
								<li>The type of join we want to use</li>
								<li>The transforms to the input topics we need to use as inputs to the join</li>
							</ol>

							Let's now dig into the specifics of how we design this join with the Kafka Streaming API DSL.

						</p>
						<p>
							<h3>Thinking Through the Streaming API DSL</h3>

							We've decided on a base representation for both topics, but there is a catch.

							Joins (on most any system) require key (both key type and value) to match up records and the two input topics are not both keyed on the object class name (<i>remember: topic key design is orthogonal to message schema design</i>).

							To build the join between the <Code>mysql_tables_jdbc_inventory</code> topic and the <code>aggregate_cart_objects</code>  topic we need an intermediate representation of the <Code>mysql_tables_jdbc_inventory</code> topic keyed on the class name of the inventory item. As we established above, fortunately <code>aggregate_cart_objects</code> was built with the class name of the item as they <code>String</code> key, as seen in the diagram below.

						</p>

						<p>
													
							<img src="./images/pre_join_topics.png" style="width: 729px; height: 360px;" />
						</p>


						<p>
							

							However, our <Code>mysql_tables_jdbc_inventory</code> topic has a null key (as explained previously in this article and also seen visually in the diagram above), so we have to re-key the topic with an intermediate representation that has a <code>String</code> key representing the item class name as well.
						</p>

<div style="border: 1px solid #999999; background-color: #EEEEEE; padding: 16px; margin: 32px;">
							For more information on the basics of how to build joins check out <a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/">"Distributed, Real-time Joins and Aggregations on User Activity Events using Kafka Streams"</a> (this article also has some great information on why the design of the Kafka Streaming Platform is so efficient) and the associated <a href="https://github.com/confluentinc/kafka-streams-examples/blob/5.0.0-post/src/main/java/io/confluent/examples/streams/PageViewRegionExample.java">example code</a>.
</div>						
						<p>
							
							

							We need to change the key of each record in the <Code>mysql_tables_jdbc_inventory</code> topic and store the results in a intermediate <code>KStream</code> representation before we can do the join. 

<!--
							The catch is that <code>KTable</code>s each have a distinct key, so we cannot change the key in place. However, a <Code>KStream</code> can do this (as they do not have the unique key constraint) so our general strategy becomes:
-->

							<ol>
								
								<li>Use the <Code>.map()</code> on the <Code>mysql_tables_jdbc_inventory</code> <code>KStream</code> representation to map the K/V pairs into a new key-space based on the name of the cart object</li>
								<li>Perform the join between the <code>KStream</code> and the <code>KTable</code> to produce a new <code>KStream</code></li>
								<li>Re-group the KStream joined data in the result with the same key with the <Code>.groupByKey()</code> method</li>
								<li>Use a reduce function to re-create a newly keyed table from the grouped stream</li>
								<li>Send this result <Code>KTable</code> to the <code>top_cart_upsells</code> topic</li>

							</ol>

							We can see the join schema from above updated in the diagram below showing how we've re-keyed the inventory <Code>KTable</code> to be keyed on the object name but now represented as a <code>KStream</code>.



						</p>

						<p>
													
							<img src="./images/bcd_joined_schemas_final.png" style="width: 590px; height: 455px;" />
						</p>
						<p>
							We now have a general DSL flow for what we want to do to create our output topic. Let's now take a more detailed look at the nuances of joining the two topics together once our <Code>mysql_tables_jdbc_inventory</code> topic is re-keyed on the object name.


						</p>


						<p>
							<h3>KStream-KTable Join</h3>

							A few notes on the KStream-KTable join:

							<ul>

								<li>they are non-windowed joins always</li>
								<li>allow the user to perform lookups against a table (Changelog stream)</li>
								<li>triggered on the event of recieving a new record from the KStream</li>


							</ul>

							They are typically used when we want to enrich a data stream with events from a lookup table.
							

<!--
							Given that we want to do a <a href="https://docs.confluent.io/current/streams/javadocs/org/apache/kafka/streams/kstream/KTable.html#join-org.apache.kafka.streams.kstream.KTable-org.apache.kafka.streams.kstream.ValueJoiner-">KTable to KTable join</a>, we'll use a basic <a href="https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#ktable-ktable-join">inner join of one <code>KTable</code> with another <code>KTable</code></a>. The result will be another ever-updating <code>KTable</code> that represents the "current" resutls of the join. 
						-->

							The general pattern is shown in the Java 7 code example below:
						</p>

<pre><code>KStream<String, Long> left = ...;
KTable<String, Double> right = ...;

...

// Java 7 example
KStream<String, String> joined = left.join(right,
    new ValueJoiner<Long, Double, String>() {
      @Override
      public String apply(Long leftValue, Double rightValue) {
        return "left=" + leftValue + ", right=" + rightValue;
      }
    },
    Joined.keySerde(Serdes.String()) /* key */
      .withValueSerde(Serdes.Long()) /* left value */
  );

</code></pre>							

						<!--
						<p>
							So in the code example above, we see a left and right record value coming in to be combined into a single value as the output of the <code>ValueJoiner.apply(...)</code> method. So how does it know which records to submit to this function to join together? It matches keys for the records together as the join is key-based, i.e. with the join predicate <code>leftRecord.key == rightRecord.key</code>. Our immediate worry is that we did not send our data to the topic with the correct key to be joined with, but we quickly realize that we can do intermediate transformations to materialize the proper view of the data for the join with the Kafka Streams DSL.

						</p>-->
						<p>
							An interesting aspect to note is that this KStream-KTable join is similar to performing a table lookup in a streaming context. We often see this pattern occur in applications where the lookup table is updated continuously and concurrently while data we are joining against streams into our system.

						</p>
						<p>
							Another condition for the execution of this join is that the left and right input data much be <A href="https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins-co-partitioning">co-partitioned</a> as described in the documentation. This means that both topics that are providing source data for this join are partitioned and keyed in such a way that records with the same key are delivered to the same stream task during processing.

							This effectively comes down to responsible topic design, which includes considering the read patterns when we're designing a topic (or any storage pattern, for that matter).

						</p>

						
						<p>
							

							Below we see the code for the streaming join implemented in Java 7:
						</p>


							<script src="http://gist-it.appspot.com/https://github.com/gwenshap/kafka-streams-wordcount/blob/master/src/main/java/com/shapira/examples/streams/wordcount/WordCountExample.java?slice=10:20"></script>

						<p>

							<h3>Explaining the Streaming Join DSL Code</h3>

							In the code above we see ...

							<ul>

								<li>read topics</li>
								<li>re-key topics as needed to match up for join</li>
								<li>relate back to join conditions</li>				
								<li>explain join specifics</li>
								<li>explain writing join to new topic, new schema</li>

							</ul>

							Default Serde for JOINS
							https://docs.confluent.io/current/streams/developer-guide/config-streams.html#default-key-serde

							https://stackoverflow.com/questions/47712933/kstream-error-reading-and-writing-avro-records

							resetting the join streaming app

						</p>
						<p>
							Reduce Operation

Rolling aggregation. Combines the values of (non-windowed) records by the grouped key. The current record value is combined with the last reduced value, and a new reduced value is returned. 							

						</p>
						<p>
							explain preparation via DSL of the aggregate_counts topic data

						</p>
						<p>
							explain prep via DSL of the inventory topic data

						</p>


						<p>
							[ When does the join update? ]
							

						</p>

						

						<p>hashing, caching, and batching</p>
						<p>games of materialized views</p>


						<p>

							https://hbr.org/2016/02/todays-automation-anxiety-was-alive-and-well-in-1960

						</p>




					</div>
				</div>
				<!-- end of section -->


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Running the Full Demo</h2>
						<p>
							Parts of this demo have been done in the previous 4 blog posts:
							<ol>
								<li>Part 1</li>
								<li>Part 2</li>
								<li>Part 3</li>
								<li>Part 4</li>

							</ol>

							In this final demo we'll take all the demo parts from previous articles and put them together as a full end-to-end demo that builds the "green light special" system for the Big Cloud Dealz team. The reader will have seen these instructions in the previous blog articles, so in this final demo we'll focus on the specific steps and have removed the extra explanation.
						</p>
						<p>



							This demo has a number of parts to stand-up, so we explain the run instructions in 3 sections below:
						
							<ul>
								<li>Setup Kafka Infrastructure</li>
								<li>Setup MySQL and Kafka Connect</li>
								<li>Run Cart Client and Streaming Applications</li>

							</ul>

							Let's start out by getting the Kafka daemons running locally on our machine.


						</p>

						<h3>Setup Kafka Locally</h3>

						<p>
							To get Kafka platform running in a state to support the rest of the example, we need to perform the following steps:


							<ol>
								<li>Run Zookeeper</li>
								<li>Run a single Kafka broker</li>
								<li>Run the Confluent schema registry</li>
								<li>Create topic: "shopping_cart_objects"</li>
								<li>Create topic: "aggregate_cart_objects"</li>
								<li>Create topic: "top_cart_upsells"</li>
							</ol>

							The script to do these steps is as follows:
						</p>

<pre><span style="font-weight: 400; font-size: 12px;"># (1) Start Zookeeper. Since this is a long-running service, you should run it in its own terminal.
./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties

# (2) Start Kafka, also in its own terminal.
./bin/kafka-server-start ./etc/kafka/server.properties

# (3) Start the Schema Registry, also in its own terminal.
./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties

# (4) Create topic: shopping_cart_objects
./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic shopping_cart_objects

# (5) create topic: aggregate_cart_objects
./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic aggregate_cart_objects

# (6) create topic: top_cart_upsells
./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic top_cart_upsells
</span></pre>

						<p>
							Let's now move on and setup MySQL, the inventory table, and Kafka Connect to move the table into a topic in Kafka.


						</p>

						<h3>Setup MySQL and Kafka Connect</h3>

						<p>

							Now that we have Kafka up and running with our base set of topics, we need to pull in the <Code>inventory</code> table from MySQL as a topic in our Kafka cluster. We need to do 3 things to accomplish this:

							<ol>


								<li>Setup MySQL table with the provided SQL script</li>
								<li>Run kafka-connect to extract the table from MySQL and create the topic in Kafka</li>
								<li>Run the CLI avro-consumer to confirm data</li>

							</ol>



							For the purposes of this demo we're going to assume the reader can find and install MySQL on their own, and we won't re-print those instructions here.



							For the purposes of this example we'll assume you either have MySQL working locally or can <a href="https://www.mysql.com/">install it</a> on your own.

							Once you've logged into MySQL, use the following script to build the inventory table and populate it with inventory data that we'll use later on in this blog series.
						</p>
						<p>


			<pre><span style="font-weight: 400; font-size: 12px;">mysql> CREATE DATABASE big_cloud_dealz;
mysql> USE big_cloud_dealz;

mysql> CREATE TABLE inventory (
 id serial NOT NULL PRIMARY KEY,
 name varchar(100),
 upsell_item varchar(200),
 count INT,
 modified timestamp default CURRENT_TIMESTAMP NOT NULL,
 INDEX `modified_index` (`modified`)
 );

mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('cup', 'plate', 100);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('bowl', 'cup', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('fork', 'spoon', 200);

mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('spoon', 'fork', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('sportsball', 'soccer goal', 2);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('tennis racket', 'tennis ball', 10);
mysql> INSERT INTO inventory (name, upsell_item, count) VALUES ('frisbees', 'frisbee goal', 100);

</span></pre>							

						</p>
						<p>

							The above commands will do 4 things:
							<ol>
								<li>Creates a database in MySQL called <code>big_cloud_dealz</code></li>
								<li>Switches the current database to <code>big_cloud_dealz</code> in the MySQL command line tool.</li>
								<li>Creates a table called <code>inventory</code> in the <code>big_cloud_dealz</code> database.</li>
								<li>Inserts 7 records in the <code>inventory</code> table that we'll use in this example.</li>

							</ol>

							Now let's move on to configuring Kafka Connect so we can ingest the <code>inventory</code> table into a topic in the Kafka cluster.

						</p>						


						<p>


							Given that our system is based on the Confluent platform for Kafka, we already have Kafka Connect setup. Before we startup Kafka Connect we need to configure it to know where our database is located, what information to ingest, and how to connect to it.
						</p>
						<p>
							

							When we run Kafka-connect, it will automatically create the topic in the Kafka cluster for us.

							The basic pattern for <A href="https://docs.confluent.io/current/connect/userguide.html#running-workers">running Kafka Connect in standalone mode</a> is:

<pre><span style="font-weight: 400; font-size: 12px;">bin/connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties ...]</span></pre>
						</p>
						<p>

							The first parameter in the <Code>connect-standalone</code> command is always a <a href="https://docs.confluent.io/current/connect/userguide.html#connect-configuring-workers">configuration file</a> for the worker. Each additional configuration file are the configration files for connectors. We need 2 basic configuration files for this example:

							<ol>
								<li>The work properties file: connect-avro-standalone.properties</li>
								<li>The JDBC database conncetor: mysql_ingest.properties</li>

							</ol>

							Below we show the contents of each of these files, and we start with the connect-avro-standalone.properties file.

						</p>

						<p>


			<pre><span style="font-weight: 400; font-size: 12px;"># Bootstrap Kafka servers. If multiple servers are specified, they should be comma-separated.
bootstrap.servers=localhost:9092

# The converters specify the format of data in Kafka and how to translate it into Connect data.
# Every Connect user will need to configure these based on the format they want their data in
# when loaded from or stored into Kafka
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://localhost:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081

# The internal converter used for offsets and config data is configurable and must be specified,
# but most users will always want to use the built-in default. Offset and config data is never
# visible outside of Connect in this format.
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# Local storage file for offset data
offset.storage.file.filename=/tmp/connect.offsets

plugin.path=share/java,/Users/josh/Documents/workspace/PattersonConsulting/confluent/mysql-connector-java-8.0.12</span></pre>
						</p>
						<p>
							

							We'll use the stock JDBC connector that ships with the Confluent platform. The stock JDBC connector allows us to connect to any relational database that supports JDBC.
							We share our Kafka Connect MySQL connector conf file (mysql_ingest.properties) below:
						</p>
						<p>

			<pre><span style="font-weight: 400; font-size: 12px;">name=test-mysql-jdbc
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1
connection.url=jdbc:mysql://localhost:3306/big_cloud_dealz?user=root&password=1234&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC
mode=timestamp+incrementing
incrementing.column.name=id
timestamp.column.name=modified
topic.prefix=mysql_tables_jdbc_
table.whitelist=inventory</span></pre>
							
						</p>
						<p>


							Using the connect-standalone command from above and the two configuration files, we can now start the ingest process. We start Kafka Connect to extract the <code>inventory</code> table from the MySQL database into the Kafka topic <Code>mysql_tables_jdbc_inventory</code> with the command:

						</p>
						<p>

<!--
			<pre><span style="font-weight: 400; font-size: 12px;">$ connect-standalone /mnt/etc/connect-avro-standalone.properties \
    /mnt/etc/mysql.properties /mnt/etc/hdfs.properties &amp;</span></pre>	
-->
<!--
/Users/josh/Documents/workspace/PattersonConsulting/confluent/kafka_tf_object_detection/src/main/resources/kafka/connect/connect-avro-standalone.properties

/Users/josh/Documents/workspace/PattersonConsulting/confluent/kafka_tf_object_detection/src/main/resources/kafka/connect/mysql_ingest.properties

-->
<!--
bin/connect-standalone worker.properties connector1.properties [connector2.properties connector3.properties ...]
	-->

			<pre><span style="font-weight: 400; font-size: 12px;">$ connect-standalone [path_to_confs]/connect-avro-standalone.properties \
    [path_to_confs]/mysql_ingest.properties</span></pre>	


						</p>
						<p>

							Now that we have the Kafka infrastructure running, topics created, and the inventory table ingested as a topic, we're ready to run our streaming applications.

						</p>





						<h3>Run Cart Application and Streaming Applications</h3>

						<p>
							Now we want to run the application code to detect the cart objects, aggregate them accross all carts, and then join them together with the inventory data to give real-time upsell information to the Big Cloud Dealz floor staff. The specific steps we need to do are:



							<ol>
								
								<li>Run Kafka aggregator streaming DSL application</li>
								<li>Run Kafka streaming-join application</li>
								<li>Run Cart 2.0 object detection client application</li>
								<li>Run CLI clients to confirm topics are being populated</li>
								
								

							</ol>

							We break down those specific instructions below.

						</p>
						<p>

<pre><span style="font-weight: 400; font-size: 12px;"># (1) Start the Streaming Aggregation App
mvn exec:java -Dexec.mainClass=com.pattersonconsultingtn.kafka.examples.tf_object_detection.StreamingDetectedObjectCounts

# (2) Start the Streaming Join App
mvn exec:java -Dexec.mainClass=com.pattersonconsultingtn.kafka.examples.tf_object_detection.StreamingJoin_CartCountsAndInventoryTopics

# (3) Run the producer from maven
mvn exec:java -Dexec.mainClass="com.pattersonconsultingtn.kafka.examples.tf_object_detection.ObjectDetectionProducer" \
  -Dexec.args="10 http://localhost:8081 ./src/main/resources/cart_images/"

# (4) Check topics
bin/kafka-console-consumer --topic aggregate_cart_objects --from-beginning \
--new-consumer --bootstrap-server localhost:9092 \
--property print.key=true \
--property print.value=true \
--formatter kafka.tools.DefaultMessageFormatter \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span></pre>
							
						</p>

<div style="border: 1px solid #999999; background-color: #EEEEEE; padding: 16px; margin: 32px;">

	<h3>Resetting Streaming Applications</h3>

	If you run the above stream applications and then try to re-run them later, they will not automatically re-process the data. To reset the application state do the following command:

	<code>./bin/kafka-streams-application-reset --application-id pct-cv-streaming-join-counts-inventory-app-3 --input-topics mysql_tables_jdbc_inventory,aggregate_cart_objects</code>


	For more information on resetting streaming applications, check out these resources:

	<ul>
		<li>https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/</li>
		<li>https://kafka.apache.org/20/documentation/streams/developer-guide/app-reset-tool.html</li>
	</ul>

</div>

					<p>
						Checking the top cart upsell topic:

<consoleoutput>
./bin/kafka-avro-console-consumer --zookeeper localhost:2181 --topic top_cart_upsells --from-beginning
Using the ConsoleConsumer with old consumer is deprecated and will be removed in a future major release. Consider using the new consumer by passing [bootstrap-server] instead of [zookeeper].
{"item_name":"cup","upsell_item_name":"plate","item_cart_count":1}
{"item_name":"fork","upsell_item_name":"spoon","item_cart_count":2}
{"item_name":"sports ball","upsell_item_name":"soccer goal","item_cart_count":11}
{"item_name":"tennis racket","upsell_item_name":"tennis ball","item_cart_count":2}
{"item_name":"frisbee","upsell_item_name":"frisbee goal","item_cart_count":8}						
</consoleoutput>

					</p>

					<p>
						Explanation of how the information would be used -- sales manager watches a custom dashboard that updates with store data, sends out sales associate with a bullhorn to the speciifc area of the store for the upsell.

					</p>



					</div>
				</div>
				<!-- end of section -->


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Summary</h2>
						<p>
						[ todo ]
						</p>



					</div>
				</div>
				<!-- end of section -->


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Footnotes</h2>
						<p>
							<OL>
								<LI>In popular culture, <a href="https://en.wikipedia.org/wiki/Kmart">Kmart became known for its "Blue Light Specials."</a> These occurred at surprise moments when a store worker would light up a mobile police light and offer a discount in a specific department of the store, while announcing the discounted special over the store's public address system. (I'm really dating myself with this reference.)</LI>
								<li>Streaming apps tended to focus on doing something with business logic (transactional) vs building off-line batch model training (batch / OLAP) jobs</li>
								<li>OLAP / MapReduce / Spark scan of all input to cache the output / predictions in a new table ("games of materialized views"). Streaming applications allow applied ML applications to potentially work more efficiently (trading disk space for ad-hoc computing power) if they are deployed as event-driven applications</li>
								<li>
Three key papers for recent state-of-the-art object detection are:
<ul>
	<li><a href="https://arxiv.org/abs/1506.01497">Faster R-CNN</a></li>
	<li><a href="https://arxiv.org/abs/1512.02325">SSD: Single Shot MultiBox Detector</a></li>
	<li><A href="https://arxiv.org/abs/1612.08242">YOLO v2</a></li>
</ul>


								</li>
								<li>a frozen graph proto with weights baked into the graph as constants (frozen_inference_graph.pb) to be used for out of the box inference</li>




							</OL>

						</p>
						
					</div>
				</div>
				<!-- end of section -->				



			</div>
		</div>






		<!-- Slider -->
		<!--
		<footer id="fh5co-footer" role="contentinfo">
			<div class="container">
				<div class="row row-bottom-padded-sm">
					<div class="col-md-4 col-sm-12">
					</div>
					<div class="col-md-3 col-md-push-1 col-sm-12 col-sm-push-0">
						<div class="fh5co-footer-widget">
						</div>
					</div>
					<div class="col-md-3 col-md-push-2 col-sm-12 col-sm-push-0">
						
						<div class="fh5co-footer-widget">
							<h3>Follow us</h3>
							<ul class="fh5co-social">
								<li class="twitter"><a href="https://twitter.com/PattersonCnsltg"><i class="icon-twitter"></i></a></li>
								<li class="linkedin"><a href="https://www.linkedin.com/in/joshlpatterson/"><i class="icon-linkedin"></i></a></li>
								<li class="message"><a href="mailto:josh@pattersonconsultingtn.com"><i class="icon-mail"></i></a></li>
							</ul>
						</div>
					</div>

				</div>

			</div>
		</footer>
	-->
	</div>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-chevron-down"></i></a>
	</div>
	
	<script src="../js/jquery.min.js"></script>
	<script src="../js/jquery.easing.1.3.js"></script>
	<script src="../js/bootstrap.min.js"></script>
	<script src="../js/owl.carousel.min.js"></script>
	<script src="../js/main.js"></script>

	</body>
</html>
