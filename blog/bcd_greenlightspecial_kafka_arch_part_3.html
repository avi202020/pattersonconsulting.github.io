
<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
	<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119541534-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119541534-1');
</script>
		
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Patterson Consulting: Blog - Real-Time Analysis of Computer Vision Objects with TensorFlow and Apache Kafka</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="blog page for Patterson Consulting" />
	<meta name="keywords" content="blog, patterson consulting, deep learning, machine learning, apache hadoop, apache spark, etl, consulting" />
	<meta name="author" content="Patterson Consulting" />

  	<!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<!-- <link rel="shortcut icon" href="favicon.ico"> -->
	
	<link rel="stylesheet" href="../css/animate.css">
	<link rel="stylesheet" href="../css/bootstrap.css">
	<link rel="stylesheet" href="../css/icomoon.css">

	<link rel="stylesheet" href="../css/owl.carousel.min.css">
	<link rel="stylesheet" href="../css/owl.theme.default.min.css">

	<link rel="stylesheet" href="../css/style.css">

	<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
	<style>
		a { 
			color: #FF0000; 
			text-decoration: underline;
		}

	</style>

	<script src="../js/modernizr-2.6.2.min.js"></script>
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body class="boxed">
	<!-- Loader -->
	<div class="fh5co-loader"></div>

	<div id="wrap">

	<div id="fh5co-page">
		<header id="fh5co-header" role="banner">
			<div class="container">
				<a href="#" class="js-fh5co-nav-toggle fh5co-nav-toggle dark"><i></i></a>
				<div id="fh5co-logo"><a href="index.html"><img src="../images/website_header_top_march2018_v0.png" ></a></div>
				<nav id="fh5co-main-nav" role="navigation">
					<ul>
						<li><a href="../about.html">About</a></li>
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Services</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../offerings/data_science_offerings.html">Data Science Offerings</a></li>
										<li><a href="../big_data_apps.html">Hadoop Applications</a></li>
										<li><a href="../vision_apps.html">Computer Vision Applications</a></li>
										<li><a href="../sensor_apps.html">Sensor Applications</a></li>
										<li><a href="../exec_strategy.html">Executive Strategy</a></li>

									</ul>
								</div>
							</div>
						</li>
						<!--
						<li><a href="portfolio.html">Portfolio</a></li>
-->
						<li class="has-sub">
							<div class="drop-down-menu">
								<a href="#">Technologies</a>
								<div class="dropdown-menu-wrap">
									<ul>
										<li><a href="../deep_learning.html">Deep Learning</a></li>
										<li><a href="../hadoop.html">Apache Hadoop</a></li>										
									</ul>
								</div>
							</div>
						</li>
						
						<li><a href="../blog/blog_index.html">Blog</a></li>
					
						<li class="cta"><a href="../contact.html">Contact</a></li>
					</ul>
				</nav>
			</div>
		</header>
		<!-- Header -->


		
		<div id="fh5co-intro" class="fh5co-section">
			<div class="container">


				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h1>Building the Next-Gen Retail Experience with Apache Kafka and Computer Vision (Part 3)</h1>
						<p>Author: <a href="http://www.twitter.com/jpatanooga">Josh Patterson</a></p>
						<p>Date: July 16th, 2018</p>
						
						<p>
							<i>In this blog article we take on the perspective of the enterprise development team of a fictional Fortune 500 Retailer "Big Cloud Dealz" looking to integrate emerging technology to re-invent the in-store customer experience. We wanted to tell the story of how the <a href="../offerings/data_science_offerings.html">Patteson Consulting team would help</a> a fictional enterprise navigate the evolution towards digital transformation.</i>

						</p>


					</div>
				</div>
				<!-- end of section -->

				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h1>Distributed Real-Time Aggregations for the Green Light Special with Kafka Streams</h1>


						<p>

							
<!--
						<div style="float: right; margin: 12px; border: 0px solid #999999;">
-->
							
<!--
						</div>
-->

							Now that we have a general plan on how to generate the shopping cart data, we need to look at the specifics of how to ingest and aggregate the detected cart objects across all baskets in the store in real-time.

							The Big Cloud Dealz team knows they have to ingest all of these predictions with Kafka from the shopping cart embedded android devices, and that they need to write some <a href="https://docs.confluent.io/current/clients/producer.html">Kafka Producer code</a> on the Android device to send data to Kafka. Beyond that, they know they need:<br/><br/>

						<div style="float: right; margin: 12px; border: 0px solid #999999;">

							<img src="./images/bcd_arch_part3.png" style="width: 591px; height: 373px;" />

						</div>


							<ul>
								<li>Need a topic to track all incoming detections from the shopping cart devices (topic: <code>shopping_cart_objects</code>)</li>
								<li>Need a topic that gives an aggregate count of the incoming shopping cart items (topic: <code>aggregate_cart_objects</code>)</li>
								<li>Need some <a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/">stream processing code</a> to convert the incoming detections data into the aggregated counts</li>
								<!--
								<li>Need a way to pull in the inventory table and associated upsell items (topic: <code>inventory_and_upsell</code>)</li>
								<li>Need a way to take the incoming streaming aggregates and join that data with the inventory table to get the upsell item (upsell items were hand-picked by business analysts via business rules) (topic: <code>top_basket_upsells</code>)</li>
							-->



							</ul>


								From an operational standpont, a <a href="https://www.confluent.io/blog/stream-data-platform-1/">single platform</a> that combines data ingestion and an streaming API would be a lot simpler for IT to support the project, and Kafka checks those boxes as well. The BCD team was able to leverage a lot of <a href="https://www.confluent.io/blog/stream-data-platform-2/">best practices</a> in terms of cluster architecture design as well.


						</p>


						<div style="border: 1px solid #999999; background-color: #EEEEEE; padding: 16px; margin: 32px;">
							<h3>System Architectural Considerations</h3>

							<ul>
								<li>Back of the envelope calculations for <A href="https://docs.confluent.io/current/streams/sizing.html">throughput of system</a></li>
								<li>local storage needs per broker</li>
								<li>Edge nodes <a href="https://docs.confluent.io/current/kafka/deployment.html">layout</a></li>
								<li>Network connectivity</li>
								<li><a href="https://docs.confluent.io/current/installation/system-requirements.html">Machine profiles</a> for each type of node in the cluster</li>
								<li>Long-term data storage with Hadoop</li>


							</ul>

						</div>

				

					</div>
				</div>
				<!-- end of section -->


				<!-- start of section -->

				<div class="row row-bottom-padded-sm">
					<div class="col-md-12" id="fh5co-content">
						<h2>Collecting and Aggregating Detected Cart 2.0 Objects in a Kafka Topic</h2>

						<p>

							As we can see in the kafka cluster topic diagram above, the incoming object detections (e.g., "names of objects in cart") are stored in the <code>shopping_cart_objects</code> topic. We want to get an aggregate view across all of the cart objects so we know what is the hot seller at that moment.

							The BCD application team put together a Kafka streaming API application that takes these raw object counts and continuously aggregates them into the <code>aggregate_cart_objects</code> topic, using this <a href="https://github.com/confluentinc/kafka-streams-examples/blob/4.1.1-post/src/main/java/io/confluent/examples/streams/PageViewRegionExample.java">example</a> as a guide on how to build streaming aggregates with the Kafka streaming API. We see the pattern of reading from and writing to topics used throughout the gamut of kafka application design as noted in the quote from the Kafka book below:

						</p>


						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"The Kafka Streams application always reads data from Kafka topics and writes its output to Kafka topics."</i></p>
						  <p><a href="http://shop.oreilly.com/product/0636920044123.do">Narkhede, Shapira, and Palino, "Kafka: The Definitive Guide"</a></p>
						</blockquote>								

						
						<p>
							The streaming API gives a solid mix of power and complexity to get the job done for real-time data processing and allows us to quickly create business applications to react to events as they arrive. Processing sequences of events generated by an entity occur under many titles including:

							<ul>
								<li>Time-series applications</li>
								<li><a href="https://www.confluent.io/blog/event-sourcing-using-apache-kafka/">Event sourcing</a> application</li>
								<li>Webserver Log data applications</li>
								<li>Financial transactions</li>
								<li>Powergrid data</li>
								<li>Smart City data</li>
							</ul>

							All of these applications follow a similar pattern of one or more entities generating events either ad-hoc ("as they happen") or in some temporal pattern. The Kafka Platform and Streaming API is a great place to build applications that leverage and act on this data as it occurs and is ingested into the system.


						</p>


						<p>
							For this streaming application to build the real-time aggregates of the objects in the shopping carts we'll need to consider a few things:
							<ul>
								<li>Building the aggregates with the Kafka streaming API (using <Code>KTable</code>s and <code>KStream</code>s)</li>
								<li>Using Avro for the Kafka messages</li>
								<li>Use of schema registry</li>
								<li>Writing the aggregate back to the new topic <code>aggregate_cart_objects</code></li>

							</ul>
							[ fix segue ]
						</p>

						<p>
							The nice aspects of the <A href="https://www.confluent.io/blog/schema-registry-kafka-stream-processing-yes-virginia-you-really-need-one/">schema registry</a> is that it watches messages passing through for new schemas and archives each new schema. Specifically the class <a href="https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java">KafkaAvroSerializer</a> does this, as explained in the <a href="https://docs.confluent.io/current/schema-registry/docs/serializer-formatter.html">Confluent documentation</a>.

							For more details on the interplay between Avro, Kafka message passing, and the schema registry check out our <a href="http://www.pattersonconsultingtn.com/blog/avro_101_w_kafka.html">blog post on Kafka and Avro</a>. 

						</p>


						<p>

							<h3>Concepts of Building an Aggregate</h3>

							Many times (with traditional application design patterns) we'd build aggregates in a relational database with SQL, and in this case our query might look something like:<br/><br/>

							<pre><span style="font-weight: 400; font-size: 12px;">select count(*), name from inventory group by name;</span></pre>

							<br/>

							The GROUP BY clause is a SQL command that is used to group records that have the same (specific column) values, optionally used in conjunction with aggregate functions to produce summary statistics.

						<p style="border: 1px solid #999999; background-color: #EEEEEE; padding: 16px; margin: 32px;">

							We'll also note that the Kafka ecosystem includes a SQL-abstraction on top of its streaming API called <a href="https://www.confluent.io/blog/ksql-open-source-streaming-sql-for-apache-kafka/">KSQL</a> (similar to the role of Apache Hive for Hadoop). The purpose of this example is to show off the streaming API, so we'll save the KSQL version of this example for a later blog post.
						</p>
						

						</p>
						<p>


							To build our aggregates in the streaming API the concepts are generally the same, its just a matter of the specific API calls. We want to take the data from the incoming topic and reference that as a <code>KStream</code> object. From there we can <code>.map()</code> the streaming (object detection) data by the detected object string <code>class_name</code> (because it was previously partitioned with the cameraID as the key from the producer) to setup our <code>groupByKey()</code> operation. Once we have all of the detected objects grouped by <code>class_name</code> we can then count the records per group with the <code>.count()</code> method and then write this result to a new topic in the form of <code>&lt;String, Long&gt;</code> pairs. We see this design pattern in the code snippet below:

						
<pre><span style="font-weight: 400; font-size: 12px;">final KStream<String, GenericRecord> detectedObjectsKStream = builder.stream("shopping_cart_objects");

final KStream<String, GenericRecord> detectedObjectsKeyedByClassname = detectedObjectsKStream.map(new KeyValueMapper<String, GenericRecord, KeyValue<String, GenericRecord>>() {
  @Override
  public KeyValue<String, GenericRecord> apply(final String cameraID, final GenericRecord record) {            
    return new KeyValue<>(record.get("class_name").toString(), record);
  }
});

KGroupedStream<String, GenericRecord> groupedDetectedObjectStream = detectedObjectsKeyedByClassname.groupByKey();

KTable<String, Long> detectedObjectCounts = groupedDetectedObjectStream.count(); 

KStream<String, Long> detectedObjectCountsStream = detectedObjectCounts.toStream();
</span></pre>
<br/>
Two of the core classes used consistently in the design of streaming applications are the <code>KTable</code> and the <code>KStream</code> class. Below we describe their function and how they fit in with the rest of the streaming pipeline.

						</p>
						<p>
						<h3>KTable</h3>




						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is interpreted as an “UPDATE” of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered an INSERT). Using the table analogy, a data record in a changelog stream is interpreted as an UPSERT aka INSERT/UPDATE because any existing row with the same key is overwritten."</i></p>
						  <p><a href="https://docs.confluent.io/current/streams/concepts.html#streams-concepts-ktable">Streams Concepts from Confluent Documentation</a></p>
						</blockquote>





					The KTable is a collection of keyed facts that are continuously updated, but also behind the scenes there is a changelog stream that tracks of the updates backing the KTable. This abstraction allows you to perform joins and aggregations on the streaming data.

					<h3>KStreams</h3>







						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"A KStream is an abstraction of a record stream, where each data record represents a self-contained datum in the unbounded data set. Using the table analogy, data records in a record stream are always interpreted as an “INSERT” – think: adding more entries to an append-only ledger – because no record replaces an existing row with the same key. Examples are a credit card transaction, a page view event, or a server log entry."</i></p>
						  <p><a href="https://docs.confluent.io/current/streams/concepts.html#streams-concepts-kstream">Streams Concepts from Confluent Documentation</a></p>
						</blockquote>

						So effectively the <code>KTable</code> gives us the latest view of a stream of events by key, and the <code>KStream</code> view gives us the full set of information, not just the latest version. Tables and streams can be converted back and forth between one another in the streaming API as well. We'll also note the use of the <a href="https://kafka.apache.org/10/javadoc/org/apache/kafka/streams/kstream/KGroupedStream.html"><code>KGroupedStream</code></a> class that is produced after a grouping (<code>.groupByKey()</code>) of a <code>KStream</code>. We typically will see this class produced as an intermediate result before an aggregation is applied (<Code>.count()</code>) resulting in a <code>KTable</code>. Many times we'll only use it in a chain of methods, but in this code listing we wanted to break out each operation separately to walk through the distinct stages.


						</p>


						<p>

							Let's put the aggregate code to work in a real-world streaming API example with the <a href="https://github.com/pattersonconsulting/kafka_tf_object_detection/blob/master/src/main/java/com/pattersonconsultingtn/kafka/examples/tf_object_detection/StreamingDetectedObjectCounts.java">StreamingDetectedObjectCounts.java</a> code listing below:


							<script src="http://gist-it.appspot.com/https://github.com/pattersonconsulting/kafka_tf_object_detection/blob/master/src/main/java/com/pattersonconsultingtn/kafka/examples/tf_object_detection/StreamingDetectedObjectCounts.java?slice=106:302"></script>

							As we can see in the full code above the streaming API code occurs in after the Kafka configuration setup area where we configure things such as the bootstrap server's address and zookeeper's address. A few areas we'll note that are different than the producer code API from part 2's example:



							<ol>
								<li>We're reading from the same topic that we wrote the detected objects to at the start of our streaming code</li>
								<li>Again we're <A href="https://www.confluent.io/blog/decoupling-systems-with-apache-kafka-schema-registry-and-avro/">leveraging Avro (with the Schema Registry)</a> for the message format and <a href="http://www.pattersonconsultingtn.com/blog/avro_101_w_kafka.html">using the GenericRecord API</a></li>
								<li>Configuring the stream application with the <code>StreamsBuilder</code></li>
								<li>We are splitting off a side stream that writes Avro messages to console so the user can see what is coming through as it is processed</li>
								<li>Writing back the aggregated results to a new topic <code>aggregate_cart_objects</code></li>
								<li>Building the streaming applicaiton topology with the <code>KafkaStreams</code> builder</li>
								<li>Finally running the streaming application with the with the <code>KafkaStreams.start()</code> method</li>
								
							</ol>


						</p>

						<p>
							<h3>Building the Application</h3>

							Running this project locally will require both git and Apache Maven to be installed. Using the <kbd>git</kbd> command locally on your computer, download the project and compile with the comands:
						</p>

<pre><code>git pull git@github.com:pattersonconsulting/kafka_tf_object_detection.git
cd kafka_tf_object_detection
mvn package
</code></pre>

						<p>

							These commands will build an uber jar in the <code>./target</code> subdirectory. (The reader may note that we're using the same <a href="https://github.com/pattersonconsulting/kafka_tf_object_detection/blob/master/pom.xml">pom.xml</a> file for this project as we did in the last project as all parts of the application are running from the same uber jar for demo simplicity.)



						</p>
						<p>
							<h3>Running the Application</h3>

							Running the application is dependent on <a href="https://www.confluent.io/download/">downloading the Confluent open source platform</a> from their website. We want to start each of the commands below in its own terminal window for simplicity.



						</p>

<pre><code># (1) Start Zookeeper. Since this is a long-running service, you should run it in its own terminal.
$ ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties

# (2) Start Kafka, also in its own terminal.
$ ./bin/kafka-server-start ./etc/kafka/server.properties

# (3) Start the Schema Registry, also in its own terminal.
./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties

# (4) create topic for incoming objects

./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic shopping_cart_objects

# create topic for aggregate counts

./bin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic aggregate_cart_objects

# (5) Start the Streaming App
mvn exec:java -Dexec.mainClass=com.pattersonconsultingtn.kafka.examples.tf_object_detection.StreamingDetectedObjectCounts

# (6) Run the producer from maven

mvn exec:java -Dexec.mainClass="com.pattersonconsultingtn.kafka.examples.tf_object_detection.ObjectDetectionProducer" \
  -Dexec.args="10 http://localhost:8081 ./src/main/resources/cart_images/"

# (7) kafka consumer setup from console

bin/kafka-console-consumer --topic aggregate_cart_objects --from-beginning \
--new-consumer --bootstrap-server localhost:9092 \
--property print.key=true \
--property print.value=true \
--formatter kafka.tools.DefaultMessageFormatter \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
</code></pre>

						<p>

						If we look at the console output from the command:
</p>
<pre><code>mvn exec:java -Dexec.mainClass="com.pattersonconsultingtn.kafka.examples.tf_object_detection.ObjectDetectionProducer" \
  -Dexec.args="10 http://localhost:8081 ./src/main/resources/cart_images/"</code></pre>
<p>
  We'll see something like:						
</p>

<consoleoutput>
* /Users/josh/Documents/workspace/PattersonConsulting/confluent/kafka_tf_object_detection/./src/main/resources/cart_images/basket_test_22.jpg
	Found sports ball          (score: 0.9834)
Box: 0.0042404234, 0.42308074, 0.3421962, 0.72577053
	Found sports ball          (score: 0.9471)
Box: 0.02893363, 0.6947326, 0.37699723, 0.99258703
Sending avro object detection data for: basket_test_22.jpg
Sending avro object detection data for: basket_test_22.jpg							
</consoleoutput>

						<p>
							Here we see the TensorFlow code finding objects in the images in the local directory included in the project <code>resources/</code> subdirectory. It will take these objects and individually send them to the Kafka cluster as messages to the <code>shopping_cart_objects</code> topic.
						</p>


						<p>
							If we switch over to the console where we have teh streaming application running with the command
							</p>


<pre><code>mvn exec:java -Dexec.mainClass=com.pattersonconsultingtn.kafka.examples.tf_object_detection.StreamingDetectedObjectCounts</code></pre>

						<p>
							We will see console output similar to below:
						</p>

<consoleoutput>
debug: frisbee
debug: frisbee
debug: sports ball
debug: frisbee
debug: frisbee
debug: clock
debug: frisbee
debug: frisbee
debug: sports ball
key=clock, value=1
key=frisbee, value=7
key=sports ball, value=5
</consoleoutput>
						<p>
							We're seeing the processing code detecting new objects as they come in, and then updating the keys in the topic <code>aggregate_cart_objects</code>.

						</p>
						<p>
							Finally, if we check out the contents of the <code>aggregate_cart_objects</code> topic from the command line, we'll see it being update with the command:

						</p>

<pre><code>bin/kafka-console-consumer --topic aggregate_cart_objects --from-beginning \
--new-consumer --bootstrap-server localhost:9092 \
--property print.key=true \
--property print.value=true \
--formatter kafka.tools.DefaultMessageFormatter \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</code></pre>						

						<p>

							Showing output similar to below:

						</p>
<consoleoutput>
cup	1
dining table	1
fork	2
sports ball	1
sports ball	3
tennis racket	2
frisbee	1
clock	1
frisbee	7
sports ball	5
sports ball	11
frisbee	8						
</consoleoutput>
						<p>
							In the next section we'll take a closer look at how the two topics, <Code>shopping_cart_objects</code> and <Code>aggregate_cart_objects</code>, differ in message schemas.



						</p>

						<p>
							<h3>A Note on Topic Design vs Avro Schemas</h3>

							Topic design (with respect to key design and partitions) is orthogonal to designing an Avro schema for the message body. Both the key and the message body for a kafka message can be defined with Avro, as explained in the confluent docs:

						<blockquote class="w3-panel w3-leftbar w3-light-grey" style="padding: 16px; font-size: 12px;">
						  <p style="font-size: 14px;">
						  <i>"When sending a message to a topic t, the Avro schema for the key and the value will be automatically registered in the Schema Registry under the subject t-key and t-value, respectively, if the compatibility test passes."</i></p>
						  <p><a href="https://docs.confluent.io/current/schema-registry/docs/serializer-formatter.html">Serializer and Formatter</a></p>
						</blockquote>

							With <a href="https://www.confluent.io/blog/put-several-event-types-kafka-topic/">topic design</a> we're concerned with what types of messages will go in our topic, and then how they will be partitioned inside the topic (affecting other downstream processing, potentially). Topic design ultimately directly affects what subset of messages a consumer can consume. <A href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/">Latency and memory overhead</a> can also fluctuate based on how we design our keys and partitions in topics. Key design for topics affects how a message flows through the cluster.

						</p>
						<p>

							With schema design we are not concerned with how the message moves through the cluster, yet we are now concerned with what information is in the payload of the Kafka message. As described above, both the key and the message can be defined with Avro. For more notes on using Avro in Kafka applications, check out our <a href="http://www.pattersonconsultingtn.com/blog/avro_101_w_kafka.html">blog post</a> on the topic.


						</p>
						<p>
							Earlier in this article we mentioned that the schema registry tracks all schemas as they are used to write to topics. Let's check out what schemas were registered in the registry.


							 We can use the <a href="https://docs.confluent.io/current/schema-registry/docs/using.html">schema registry web API</a> with the <kbd>curl</kbd> command to get a listing of all of the schemas in the registry:

						</p>

						<p>

							<pre><span style="font-weight: 400; font-size: 12px;">$ curl -X GET http://localhost:8081/subjects</span></pre>

						</p>
						<p>

<consoleoutput>
["shopping_cart_objects-key","shopping_cart_objects-value","KafkaStreamingCartObjectAggregatorApp-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition-value"]
</consoleoutput>
						</p>
						<p>
							We notice that our aggregate topic <code>aggregate_cart_objects</code> does not show up in the Schema Registry list whereas the <code>shopping_cart_objects</code> topic does show up for both key (<code>shopping_cart_objects-key</code>) and value (<code>shopping_cart_objects-value</code>) Avro schemas. This is because when we first wrote to the topic in the streaming application <code>StreamingDetectedObjectCounts</code> we manually specified the Serdes for both the key (<code>Serdes.String()</code>) and the value (<code>Serdes.Long()</code>) of the topic with the code:
						</p>

<pre><code>detectedObjectCountsStream.to( aggregateDestTopicName, Produced.with(Serdes.String(), Serdes.Long()));
</code></pre>
						<p>

							Had we not specified these settings, the defaults set in the application configuration would have specified Avro as the Serde.

						</p>
<!--
						<p>

<consoleoutput>
./bin/kafka-topics --list --zookeeper localhost:2181
KafkaStreamingCartObjectAggregatorApp-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog
KafkaStreamingCartObjectAggregatorApp-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition
__confluent.support.metrics
__consumer_offsets
_schemas
aggregate_cart_objects
shopping_cart_objects
</consoleoutput>

						</p>	
						-->					
						<p>
							<h3>Summary</h3>

							At this point we have detected objects streaming from our shopping cart into <code>shopping_cart_objects</code> and then a real-time streaming application is building an aggregate of that information in the <code>aggregate_cart_objects</code> topic. However, we're still not ready to launch this application to power the BCD team's "Green Light Special" retail system. Now we need to join this aggregated information in real-time with the current inventory of items from BCD's inventory counts in their MySQL database. We'll do just that in part 4 of this series, stay tuned.

						</p>


						


					</div>
				</div>
				<!-- end of section -->



			</div>
		</div>






		<!-- Slider -->
		<!--
		<footer id="fh5co-footer" role="contentinfo">
			<div class="container">
				<div class="row row-bottom-padded-sm">
					<div class="col-md-4 col-sm-12">
					</div>
					<div class="col-md-3 col-md-push-1 col-sm-12 col-sm-push-0">
						<div class="fh5co-footer-widget">
						</div>
					</div>
					<div class="col-md-3 col-md-push-2 col-sm-12 col-sm-push-0">
						
						<div class="fh5co-footer-widget">
							<h3>Follow us</h3>
							<ul class="fh5co-social">
								<li class="twitter"><a href="https://twitter.com/PattersonCnsltg"><i class="icon-twitter"></i></a></li>
								<li class="linkedin"><a href="https://www.linkedin.com/in/joshlpatterson/"><i class="icon-linkedin"></i></a></li>
								<li class="message"><a href="mailto:josh@pattersonconsultingtn.com"><i class="icon-mail"></i></a></li>
							</ul>
						</div>
					</div>

				</div>

			</div>
		</footer>
	-->
	</div>
	</div>

	<div class="gototop js-top">
		<a href="#" class="js-gotop"><i class="icon-chevron-down"></i></a>
	</div>
	
	<script src="../js/jquery.min.js"></script>
	<script src="../js/jquery.easing.1.3.js"></script>
	<script src="../js/bootstrap.min.js"></script>
	<script src="../js/owl.carousel.min.js"></script>
	<script src="../js/main.js"></script>

	</body>
</html>
